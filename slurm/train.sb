#!/bin/sh
#SBATCH --job-name=arc1_full_lexi_3_(500g_0.18e)
#SBATCH --partition=gpu-a100-q
#SBATCH --gres=gpu:a100:1
#SBATCH --cpus-per-task=8
#SBATCH --mem=8G
#SBATCH --time=07-00:00:00
#SBATCH --output=logs/train/%x-%j.out
#SBATCH --error=logs/train/%x-%j.err

#* CALL THIS FROM slurm/

module load python39
module load pytorch-py39-cuda11.8-gcc11
module load pytorch-extra-py39-cuda11.8-gcc11

mkdir -p logs/train ../checkpoints

python3 ../train.py full_lexi --name "$SLURM_JOB_NAME" --dataset "arc1" --epsilon 0.18 --epochs 100